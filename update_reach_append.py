{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37741f76-f767-4331-85c6-4f9414561114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update (append since last timestamp → end time)\n",
      "Reaches: 842\n",
      "End time: 2026-02-25T02:48:19Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3749327/3824141119.py:130: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts2 = pd.to_datetime(s[ts.isna()], errors=\"coerce\", utc=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/842] 22511300103 no change (all timestamps already present)\n",
      "[2/842] 22400900035 no change (all timestamps already present)\n",
      "[3/842] 22601000045 no change (0 rows returned)\n",
      "[4/842] 22511100015 no change (0 rows returned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3749327/3824141119.py:130: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts2 = pd.to_datetime(s[ts.isna()], errors=\"coerce\", utc=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/842] 22511100055 no change (all timestamps already present)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3749327/3824141119.py:130: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts2 = pd.to_datetime(s[ts.isna()], errors=\"coerce\", utc=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/842] 22511100021 no change (all timestamps already present)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 267\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    205\u001b[39m params = {\n\u001b[32m    206\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mReach\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_id\u001b[39m\u001b[33m\"\u001b[39m: rid,\n\u001b[32m   (...)\u001b[39m\u001b[32m    212\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfields\u001b[39m\u001b[33m\"\u001b[39m: FIELDS,\n\u001b[32m    213\u001b[39m }\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mSLEEP_BETWEEN_REQUESTS_S\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     r = request_with_retries(HYDROCRON_URL, params)\n\u001b[32m    218\u001b[39m     new_df = hydrocron_response_to_df(r.text)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script will be automated to run daily to append new Hydrocron RiverSP rows\n",
    "into existing per-reach CSVs.\n",
    "\n",
    "What this script does:\n",
    "    Reads a list of reach IDs from the GeoPackage layer\n",
    "    Looks at existing reach's CSV in the 'hydrocron_timeseries_by_reach' folder and finds the most recent timestamp already saved\n",
    "    Queries Hydrocron only for the last timestamp to now\n",
    "    Formats returned rows to match current CSVs' format and appends them\n",
    "    Prints progress and keeps per-reach log of new data, no change, or an error\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, json, time, random\n",
    "from io import StringIO\n",
    "from datetime import datetime, timezone, date, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "\n",
    "\n",
    "# SET UP #\n",
    "\n",
    "# where reach_ids within Dnipro basin are found in the clipped geopackage\n",
    "GPKG_PATH = r\"./dnipro_sword_reaches_clip.gpkg\"\n",
    "LAYER_NAME = \"dnipro_reaches\"\n",
    "REACH_ID_FIELD = \"reach_id\"\n",
    "\n",
    "# folder where the CSVs of each reach will be saved\n",
    "OUT_DIR = r\"./hydrocron_timeseries_by_reach\"\n",
    "\n",
    "# URL and collection name. This is where the data is pulled from Hydrocron - SWOT's RiverSP data hosted on Hydrocron\n",
    "HYDROCRON_URL = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries\"\n",
    "COLLECTION_NAME = \"SWOT_L2_HR_RiverSP_2.0\"\n",
    "# variables appended into the CSVs\n",
    "FIELDS = \"reach_id,time_str,cycle_id,pass_id,wse,slope,width,dschg_gm,dschg_gm_q,reach_q\"\n",
    "\n",
    "# ensures data is dowloaded if Hyrdrocron stops responding\n",
    "# does this by timing out after 60 seconds and retries 5 times\n",
    "TIMEOUT_S = 60\n",
    "SLEEP_BETWEEN_REQUESTS_S = (0.2, 0.6)\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# re-query a bit before the last saved timestamp so you don’t miss late/edge timestamps\n",
    "OVERLAP_HOURS = 48\n",
    "\n",
    "# if a reach CSV is empty/nonexistent, only pull most recent days rather than full history\n",
    "DEFAULT_BACKFILL_DAYS_IF_EMPTY = 2\n",
    "\n",
    "\n",
    "# FUNCTIONS #\n",
    "\n",
    "\n",
    "# converts Hydrocron API response into a pandas dataframe - if the response is empty, an empty dataframe is returned\n",
    "def hydrocron_response_to_df(text: str) -> pd.DataFrame:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return pd.DataFrame()\n",
    "    if text.startswith(\"{\"):\n",
    "        try:\n",
    "            obj = json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            return pd.DataFrame()\n",
    "        csv_text = (obj.get(\"results\", {}).get(\"csv\", \"\") or \"\").strip()\n",
    "        return pd.read_csv(StringIO(csv_text)) if csv_text else pd.DataFrame()\n",
    "    return pd.read_csv(StringIO(text))\n",
    "\n",
    "\n",
    "# This function sends a request to the Hydrocron API and automatically retries if the request fails.\n",
    "def request_with_retries(url: str, params: dict) -> requests.Response:\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=TIMEOUT_S)\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                raise RuntimeError(f\"HTTP {r.status_code}\")\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            backoff = min(30, (2 ** (attempt - 1)) * 0.7) + random.uniform(0, 0.5)\n",
    "            time.sleep(backoff)\n",
    "    raise RuntimeError(f\"Failed after {MAX_RETRIES} retries. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "# cleans up reach ID\n",
    "def safe_reach_filename(reach_id: str) -> str:\n",
    "    s = str(reach_id).strip()\n",
    "    cleaned = \"\".join(ch for ch in s if ch.isalnum() or ch in (\"-\", \"_\"))\n",
    "    return cleaned or \"unknown_reach\"\n",
    "\n",
    "\n",
    "# Load unique reach IDs from the GeoPackage layer, cleaning formatting and removing null or empty values\n",
    "def load_reach_ids() -> list[str]:\n",
    "    gdf = gpd.read_file(GPKG_PATH, layer=LAYER_NAME)\n",
    "    if REACH_ID_FIELD not in gdf.columns:\n",
    "        raise ValueError(f\"Field '{REACH_ID_FIELD}' not found in layer '{LAYER_NAME}'\")\n",
    "    s = gdf[REACH_ID_FIELD].dropna().astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    return s.loc[s != \"\"].unique().tolist()\n",
    "\n",
    "\n",
    "# Load an existing reach CSV if present, otherwise return an empty DataFrame.\n",
    "def read_existing_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Extract the latest time_str value from an existing CSV as a timezone-aware UTC datetime.\n",
    "def get_last_time_dt(existing: pd.DataFrame) -> datetime | None:\n",
    "    \"\"\"\n",
    "    Return latest timestamp in existing CSV as UTC datetime.\n",
    "    Robust to empty files / missing time_str / odd formats.\n",
    "    \"\"\"\n",
    "    if existing is None or len(existing) == 0 or \"time_str\" not in existing.columns:\n",
    "        return None\n",
    "\n",
    "    s = existing[\"time_str\"].astype(str).str.strip()\n",
    "    s = s[(s != \"\") & (s.str.lower() != \"nan\")]\n",
    "\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "\n",
    "    ts = pd.to_datetime(s, format=\"%Y-%m-%dT%H:%M:%SZ\", errors=\"coerce\", utc=True)\n",
    "\n",
    "    if ts.isna().any():\n",
    "        ts2 = pd.to_datetime(s[ts.isna()], errors=\"coerce\", utc=True)\n",
    "        ts.loc[ts.isna()] = ts2\n",
    "\n",
    "    if ts.notna().any():\n",
    "        return ts.max().to_pydatetime()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Standardize new Hydrocron rows (ensure reach_id, normalize time_str, drop invalid times) for safe merging.\n",
    "def normalize_new_df(df: pd.DataFrame, rid: str) -> pd.DataFrame:\n",
    "    if df is None or len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"reach_id\" not in df.columns:\n",
    "        df[\"reach_id\"] = rid\n",
    "\n",
    "    if \"time_str\" in df.columns:\n",
    "        t = pd.to_datetime(df[\"time_str\"], errors=\"coerce\", utc=True)\n",
    "        df = df.loc[t.notna()].copy()\n",
    "        # Keep alignment safe even if rows were dropped\n",
    "        df[\"time_str\"] = t.loc[t.notna()].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\").values\n",
    "\n",
    "    return df\n",
    "\n",
    "# Combine existing and new rows, drop duplicate timestamps, and sort by time_str\n",
    "def append_dedup(existing: pd.DataFrame, new: pd.DataFrame) -> pd.DataFrame:\n",
    "    if existing is None or len(existing) == 0:\n",
    "        out = new.copy()\n",
    "    elif new is None or len(new) == 0:\n",
    "        out = existing.copy()\n",
    "    else:\n",
    "        out = pd.concat([existing, new], ignore_index=True, sort=False)\n",
    "\n",
    "    if \"time_str\" in out.columns:\n",
    "        out[\"time_str\"] = out[\"time_str\"].astype(str)\n",
    "        out = out.drop_duplicates(subset=[\"time_str\"], keep=\"last\")\n",
    "        out = out.sort_values(\"time_str\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# For each reach, request new Hydrocron rows since the last saved time (with overlap), \n",
    "# append/deduplicate, write CSV, and print a summary.\n",
    "def main():\n",
    "    end_dt = datetime.now(timezone.utc)\n",
    "\n",
    "    reach_ids = load_reach_ids()\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Update (append since last timestamp → end time)\")\n",
    "    print(\"Reaches:\", len(reach_ids))\n",
    "    print(\"End time:\", end_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "\n",
    "    log = []\n",
    "    appended_count = 0\n",
    "    no_change_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for i, rid in enumerate(reach_ids, 1):\n",
    "        out_csv = os.path.join(OUT_DIR, f\"reach_{safe_reach_filename(rid)}.csv\")\n",
    "        existing = read_existing_csv(out_csv)\n",
    "\n",
    "        last_dt = get_last_time_dt(existing)\n",
    "\n",
    "        if last_dt is not None:\n",
    "            start_dt = last_dt - timedelta(hours=OVERLAP_HOURS)\n",
    "        else:\n",
    "            start_dt = end_dt - timedelta(days=DEFAULT_BACKFILL_DAYS_IF_EMPTY)\n",
    "\n",
    "        if start_dt >= end_dt:\n",
    "            start_dt = end_dt - timedelta(days=1)\n",
    "\n",
    "        params = {\n",
    "            \"feature\": \"Reach\",\n",
    "            \"feature_id\": rid,\n",
    "            \"start_time\": start_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"end_time\": end_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"output\": \"csv\",\n",
    "            \"collection_name\": COLLECTION_NAME,\n",
    "            \"fields\": FIELDS,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            time.sleep(random.uniform(*SLEEP_BETWEEN_REQUESTS_S))\n",
    "            r = request_with_retries(HYDROCRON_URL, params)\n",
    "            new_df = hydrocron_response_to_df(r.text)\n",
    "            new_df = normalize_new_df(new_df, rid)\n",
    "\n",
    "            # --- Unified \"no change\" block ---\n",
    "            # no rows came back OR nothing in the response is new vs existing time_str values\n",
    "            if new_df is None or len(new_df) == 0:\n",
    "                status = \"no_change\"\n",
    "                no_change_count += 1\n",
    "                log.append({\"reach_id\": rid, \"status\": status, \"start\": params[\"start_time\"], \"end\": params[\"end_time\"],\n",
    "                            \"new_rows\": 0, \"total_rows\": len(existing)})\n",
    "                print(f\"[{i}/{len(reach_ids)}] {rid} no change (0 rows returned)\")\n",
    "                continue\n",
    "\n",
    "            if existing is not None and len(existing) > 0 and \"time_str\" in existing.columns and \"time_str\" in new_df.columns:\n",
    "                existing_times = set(existing[\"time_str\"].astype(str).tolist())\n",
    "                incoming_times = set(new_df[\"time_str\"].astype(str).tolist())\n",
    "                truly_new = incoming_times - existing_times\n",
    "                if len(truly_new) == 0:\n",
    "                    status = \"no_change\"\n",
    "                    no_change_count += 1\n",
    "                    log.append({\"reach_id\": rid, \"status\": status, \"start\": params[\"start_time\"], \"end\": params[\"end_time\"],\n",
    "                                \"new_rows\": 0, \"total_rows\": len(existing)})\n",
    "                    print(f\"[{i}/{len(reach_ids)}] {rid} no change (all timestamps already present)\")\n",
    "                    continue\n",
    "\n",
    "            # Otherwise, append + dedup + write\n",
    "            out_df = append_dedup(existing, new_df)\n",
    "            out_df.to_csv(out_csv, index=False)\n",
    "\n",
    "            appended_count += 1\n",
    "            log.append({\"reach_id\": rid, \"status\": \"appended\", \"start\": params[\"start_time\"], \"end\": params[\"end_time\"],\n",
    "                        \"new_rows\": len(new_df), \"total_rows\": len(out_df)})\n",
    "            print(f\"[{i}/{len(reach_ids)}] {rid} appended new={len(new_df)} total={len(out_df)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            log.append({\"reach_id\": rid, \"status\": f\"error: {e}\", \"start\": params[\"start_time\"], \"end\": params[\"end_time\"],\n",
    "                        \"new_rows\": None, \"total_rows\": None})\n",
    "            print(f\"[{i}/{len(reach_ids)}] {rid} ERROR: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nSUMMARY\")\n",
    "    print(f\"  appended : {appended_count}\")\n",
    "    print(f\"  no change: {no_change_count}\")\n",
    "    print(f\"  errors   : {error_count}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
