{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1955a-e212-44d4-ab9b-cadbe069a09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Hydrocron Download (Nodes) — matches your working retrieval\n",
      "Nodes: 44006\n",
      "Window: 2022-12-01T00:00:00Z → 2026-02-24T22:12:49Z\n",
      "Workers: 16\n",
      "collection_name: SWOT_L2_HR_RiverSP_2.0\n",
      "fields: node_id,reach_id,time_str,wse,width\n",
      "done 200/44006 | ok=64 empty=0 http_fail=136 err=0\n",
      "done 400/44006 | ok=253 empty=0 http_fail=147 err=0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parallel historical download for Hydrocron NODE time series.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, json, time, random, threading\n",
    "from io import StringIO\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "\n",
    "\n",
    "GPKG_PATH = r\"dnipro_sword_nodes_clip.gpkg\"\n",
    "LAYER_NAME = \"dnipro_nodes\"\n",
    "NODE_ID_FIELD = \"node_id\"\n",
    "\n",
    "OUT_DIR = r\"hydrocron_timeseries_by_node_test\"\n",
    "\n",
    "HYDROCRON_URL = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries\"\n",
    "COLLECTION_NAME = \"SWOT_L2_HR_RiverSP_2.0\"\n",
    "FIELDS = \"node_id,reach_id,time_str,wse,width\"\n",
    "\n",
    "START_TIME = \"2022-12-01T00:00:00Z\"\n",
    "END_TIME = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "TIMEOUT_S = 60\n",
    "MAX_RETRIES = 6\n",
    "\n",
    "MAX_WORKERS = 16\n",
    "\n",
    "\n",
    "JITTER_S = (0.0, 0.15)\n",
    "\n",
    "\n",
    "_tls = threading.local()\n",
    "\n",
    "def get_session() -> requests.Session:\n",
    "    if not hasattr(_tls, \"session\"):\n",
    "        _tls.session = requests.Session()\n",
    "    return _tls.session\n",
    "\n",
    "\n",
    "def hydrocron_response_to_df(text: str) -> pd.DataFrame:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if text.startswith(\"{\"):\n",
    "        try:\n",
    "            obj = json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            return pd.DataFrame()\n",
    "        csv_text = (obj.get(\"results\", {}).get(\"csv\", \"\") or \"\").strip()\n",
    "        return pd.read_csv(StringIO(csv_text)) if csv_text else pd.DataFrame()\n",
    "\n",
    "    return pd.read_csv(StringIO(text))\n",
    "\n",
    "\n",
    "def request_with_retries(params: dict) -> requests.Response:\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            if JITTER_S != (0.0, 0.0):\n",
    "                time.sleep(random.uniform(*JITTER_S))\n",
    "\n",
    "            sess = get_session()\n",
    "            r = sess.get(HYDROCRON_URL, params=params, timeout=TIMEOUT_S)\n",
    "\n",
    "            # Retry on transient / rate limit\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                raise RuntimeError(f\"HTTP {r.status_code}\")\n",
    "\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            backoff = min(60, (2 ** (attempt - 1)) * 0.8) + random.uniform(0, 0.8)\n",
    "            time.sleep(backoff)\n",
    "\n",
    "    raise RuntimeError(f\"Failed after {MAX_RETRIES} retries. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "def safe_node_filename(node_id: str) -> str:\n",
    "    s = str(node_id).strip()\n",
    "    cleaned = \"\".join(ch for ch in s if ch.isalnum() or ch in (\"-\", \"_\"))\n",
    "    return cleaned or \"unknown_node\"\n",
    "\n",
    "\n",
    "def load_node_ids() -> list[str]:\n",
    "    gdf = gpd.read_file(GPKG_PATH, layer=LAYER_NAME)\n",
    "    if NODE_ID_FIELD not in gdf.columns:\n",
    "        raise ValueError(f\"Field '{NODE_ID_FIELD}' not found in layer '{LAYER_NAME}'\")\n",
    "\n",
    "    s = (\n",
    "        gdf[NODE_ID_FIELD]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    )\n",
    "    return s.loc[s != \"\"].unique().tolist()\n",
    "\n",
    "\n",
    "def fetch_one_node(nid: str) -> dict:\n",
    "    out_csv = os.path.join(OUT_DIR, f\"node_{safe_node_filename(nid)}.csv\")\n",
    "\n",
    "    params = {\n",
    "        \"feature\": \"Node\",\n",
    "        \"feature_id\": nid,\n",
    "        \"start_time\": START_TIME,\n",
    "        \"end_time\": END_TIME,\n",
    "        \"output\": \"csv\",\n",
    "        \"collection_name\": COLLECTION_NAME,\n",
    "        \"fields\": FIELDS,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = request_with_retries(params)\n",
    "\n",
    "        # If Hydrocron returns 400 with a JSON message, capture it in status\n",
    "        if r.status_code != 200:\n",
    "            preview = (r.text or \"\").strip().replace(\"\\n\", \" \")[:300]\n",
    "            return {\"node_id\": nid, \"rows\": None, \"status\": f\"http_{r.status_code}: {preview}\", \"csv\": out_csv}\n",
    "\n",
    "        df = hydrocron_response_to_df(r.text)\n",
    "        df.to_csv(out_csv, index=False)\n",
    "\n",
    "        return {\"node_id\": nid, \"rows\": len(df), \"status\": \"ok\" if len(df) else \"empty\", \"csv\": out_csv}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"node_id\": nid, \"rows\": None, \"status\": f\"error: {e}\", \"csv\": out_csv}\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    node_ids = load_node_ids()\n",
    "    n = len(node_ids)\n",
    "\n",
    "    print(\"Parallel Hydrocron Download (Nodes) — matches your working retrieval\")\n",
    "    print(\"Nodes:\", n)\n",
    "    print(\"Window:\", START_TIME, \"→\", END_TIME)\n",
    "    print(\"Workers:\", MAX_WORKERS)\n",
    "    print(\"collection_name:\", COLLECTION_NAME)\n",
    "    print(\"fields:\", FIELDS)\n",
    "\n",
    "    log_rows = []\n",
    "    done = ok = empty = err = http_fail = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = [ex.submit(fetch_one_node, nid) for nid in node_ids]\n",
    "\n",
    "        for idx, fut in enumerate(as_completed(futures), 1):\n",
    "            res = fut.result()\n",
    "            log_rows.append(res)\n",
    "\n",
    "            st = res[\"status\"]\n",
    "            done += 1\n",
    "            if st == \"ok\":\n",
    "                ok += 1\n",
    "            elif st == \"empty\":\n",
    "                empty += 1\n",
    "            elif st.startswith(\"http_\"):\n",
    "                http_fail += 1\n",
    "            else:\n",
    "                err += 1\n",
    "\n",
    "            if done % 200 == 0 or done == n:\n",
    "                print(f\"done {done}/{n} | ok={ok} empty={empty} http_fail={http_fail} err={err}\")\n",
    "\n",
    "    pd.DataFrame(log_rows).to_csv(\n",
    "        os.path.join(OUT_DIR, \"download_log_historical_nodes_parallel.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
